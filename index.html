---
layout: default
title: Home
---
<header>
  <img alt="{{ site.name }} picture" src="{{ site.picture }}" />
  <h1>
    <strong>Yining Hong</strong>
  </h1>

  <p>
    University of California, Los Angeles</br>
    Department of Computer Science<br>
  </p>
  <br><br>
  <nav>
    <ul>
<!--       <li><a href="https://drive.google.com/file/d/1mBJ14ZlqIxVOYjJxZRQlTpHOiclSnYRE/view?usp=sharing">CV</a></li> -->
      <li><a href="https://github.com/evelinehong">Github</a></li>
      <li><a href="https://scholar.google.com/citations?user=PTYxORcAAAAJ">Google Scholar</a></li>
      <li><a href="https://twitter.com/yining_hong">Twitter</a></li>
      <li><a href="mailto:yninghong@gmail.com">yninghong AT gmail DOT com</a></li>
<!--       <li><a href="https://evelinehong.github.io/photos.html">Gallery</a></li> -->
    </ul>
  </nav>

  </small>
</header>

<section class="content">
  <p>
    <h2>About Me</h2>

    I am currently a fourth-year PhD student in computer science at UCLA. I'm co-advised by Prof. Chuang Gan from MIT-IBM Watson AI Lab，Prof. Song-Chun Zhu and Prof. Ying Nian Wu from UCLA. Previously, I was an undergraduate in the Department of Electrical Engineering in Shanghai Jiao Tong University. 
      <br>
  </p>

  <p>
    <h2> Research Highlights</h2>
    I'm dedicated to building <strong>general-purpose embodied agents</strong> that could actively explore and interact with the <strong>3D physical world</strong>, 
    and perform common sense reasoning within the embodied environment. Specifically, I think the critical aspects of building such embodied agents reside in:
    <ul>
      <li><strong>Building 3D world model</strong>. </li>
      <li><strong>Large embodied foundation models</strong> .</li>
      <li><strong>Visual Common Sense Reasoning</strong>. </li>
    </ul>
<!--     Specifically, I'm interested in: <br>
    <ul>
      <li>Embodied Foundation Models</li>
      <li>3D Reasoning</li>
      <li>Visual Common Sensen Reasoning</li>
      My main focus lies in <strong>3D reasoning</strong>, especially about the perspectives that go beyond pattern recognition in 2D images and need to be reasoned in the 3D space, including physics, functionality etc. I'm also interested in concept learning and structure learning, which are crucial aspects of visual reasoning in the 3D space.<br>
 -->
  </p>
       
<!--   I’ve been drawing inspiration from cognitive science, and endowing machines with human-like concept learning abilities via neural-graphical-symbolic learning. Specifically:<br>
    <strong>Neural</strong>: neural networks for natural language processing and computer vision. <br>
    <strong>Graphical</strong>: graph parsing via grammar learning. <br>
    <strong>Symbolic</strong>: symbolic reasoning, including program synthesis, inductive logic programming and theorem proving. -->
  <!--     My research interest lies in Neural-Symbolic Learning, Mathematical Reasoning and Program Synthesis, particularly in the intersection of Natural Language Processing and Computer Vision. -->
  

  <p>
    <h2>News</h2>
    <ul>
      <li>2023/12: I gave a talk on "Building Embodied 3D Foundation Models" at Nvidia!</li>
      <!--<li>2023/12: I will be giving a series of talks on "Building Embodied 3D Foundation Models" at Nvidia (12/18), SJTU (12/28), Shanghai AI Lab (12/29), NUAA (01/05), Peking University (01/08), BIGAI (01/08) and Tsinghua University (01/09)!
      -->
      <li>2023/09: 3D-LLM is accepted by NeurIPS as Spotlight!</li>
      <li>2023/06: Successfully hosted the second <a href=https://mvcs-workshop.github.io/>Machine Visual Common Sense</a> Workshop at CVPR!</li>
    </ul>
  <p>
    
    <h2>Publications</h2>

    MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World <br>
    <ul class="a">
        <li>
        <strong>Yining Hong</strong>; Zishuo Zheng;  Peihao Chen; Yian Wang; Junyan Li; Chuang Gan <br>
        Preprint 2024 &nbsp;  <a href="https://vis-www.cs.umass.edu/multiply/">[Project Page]</a>&nbsp;  
        <a href="https://arxiv.org/abs/2401.08577">[Paper]</a>
        </li>
      </ul>
        <br>
    
    <strong>Compositional VLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding</strong> <br>
      <ul class="a">
        <li>
        Junyan Li; Delin Chen; <strong>Yining Hong</strong>; Zhenfang Chen, Peihao Chen; Yikang Shen; Chuang Gan <br>
        ICLR 2024 &nbsp;  <a href="https://vis-www.cs.umass.edu/CoVLM/">[Project Page]</a>&nbsp;  
        <a href="https://arxiv.org/abs/2311.03354">[Paper]</a>&nbsp;  
        <a href="https://github.com/UMass-Foundation-Model/CoVLM">[Code & Data]</a>
        </li>
      </ul>
        <br>

   <strong>Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules</strong> <br>
      <ul class="a">
        <li>
        Zhenfang Chen; Rui Sun; Wenjun Liu; <strong>Yining Hong</strong>; Chuang Gan <br>
        ICLR 2024&nbsp;  <a href="https://vis-www.cs.umass.edu/genome/">[Project Page]</a>&nbsp;  
        <a href="https://arxiv.org/abs/2311.04901">[Paper]</a>&nbsp;  
        <a href="https://github.com/UMass-Foundation-Model/genome">[Code]</a>&nbsp;
        <a href="https://connecthkuhk-my.sharepoint.com/personal/u3004417_connect_hku_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fu3004417%5Fconnect%5Fhku%5Fhk%2FDocuments%2Fdata%2Fdataset%2Ezip&parent=%2Fpersonal%2Fu3004417%5Fconnect%5Fhku%5Fhk%2FDocuments%2Fdata&ga=1">[Data]</a>
        </li>
      </ul>
        <br>
  
    <strong>3D-LLM: Injecting the 3D World into Large Language Models</strong> <br>
      <ul class="a">
        <li>
        <strong>Yining Hong</strong>; Haoyu Zhen; Peihao Chen; Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan<br>
        NeurIPS2023 (<b>Spotlight</b>)&nbsp;  <a href="https://vis-www.cs.umass.edu/3dllm/">[Project Page]</a>&nbsp;  
        <a href="https://arxiv.org/abs/2307.12981">[Paper]</a>&nbsp;  
        <a href="https://github.com/UMass-Foundation-Model/3D-LLM">[Code & Data]</a>
        </li>
      </ul>
        <br>
        

    <strong>3D Concept Learning and Reasoning from Multi-View Images</strong><br>
    <ul class="a">
      <li>
      <strong>Yining Hong</strong>; Chunru Lin; Yilun Du; Zhenfang Chen; Joshua B. Tenenbaum; Chuang Gan<br>
      CVPR2023&nbsp; <a href="https://vis-www.cs.umass.edu/3d-clr/">[Project Page]</a>&nbsp; 
      <a href="https://arxiv.org/abs/2303.11327">[Paper]</a>&nbsp; 
      <a href="https://github.com/evelinehong/3D-CLR-Official">[Code & Data]</a>
        </li>
      </ul>
    <br>

    <strong>See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning</strong><br>
    <ul class="a">
      <li>
    Zhenfang Chen, Qinhong Zhou, Yikang Shen, <strong>Yining Hong</strong>, Hao Zhang, Chuang Gan<br>
    AAAI2024&nbsp; <a href="https://arxiv.org/abs/2301.05226">[Paper]</a>
        </li>
      </ul>
    <br>

    <strong>A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics</strong><br>
     <ul class="a">
      <li>
    Qing Li; Siyuan Huang; <strong>Yining Hong</strong>; Yixin Zhu; Ying Nian Wu; Song-Chun Zhu<br>
    ICLR2023 &nbsp;<a href="https://liqing-ustc.github.io/HINT/">[Project Page]</a>&nbsp;
    <a href="https://arxiv.org/abs/2103.01403">[Paper]</a>&nbsp;
    <a href="https://github.com/liqing-ustc/HINT">[Code]</a>&nbsp;
    <a href="https://github.com/liqing-ustc/HINT/raw/master/data/dataset.tar.xz">[Data]</a>
         </li>
      </ul><br>

    <strong>3D Concept Grounding on Neural Fields</strong><br>
  <ul class="a">
     <li>
    <strong>Yining Hong</strong>; Yilun Du; Chunru Lin; Joshua Tenenbaum; Chuang Gan<br>
    NeurIPS2022 &nbsp;<a href="https://arxiv.org/abs/2207.06403">[Paper]</a>&nbsp;
    <a href="https://github.com/evelinehong/3D-Concept-Grounding">[Code & Data]</a>
        </li>
      </ul><br>

    <strong>Fixing Malfunctional Objects With Learned Physical Simulation and Functional Prediction</strong><br>
  <ul class="a">
     <li>
    <strong>Yining Hong</strong>; Kaichun Mo; Li Yi; Leonidas Guibas; Antonio Torralba; Joshua Tenenbaum; Chuang Gan<br>
    CVPR2022 &nbsp;<a href="https://arxiv.org/abs/2205.02834">[Paper]</a>&nbsp;
    <a href="https://github.com/evelinehong/FixIt">[Code & Data]</a>
       </li>
      </ul><br>
    
  <strong>PTR: A Benchmark for Part-based Conceptual, Relational, and Physical Reasoning</strong><br>
  <ul class="a">
     <li>
    <strong> Yining Hong</strong>; Li Yi; Joshua B. Tenenbaum; Antonio Torralba; Chuang Gan<br>
    NeurIPS2021 &nbsp;<a href="https://arxiv.org/abs/2112.05136">[Paper]</a>&nbsp;
    <a href="https://github.com/evelinehong/PTR">[Code & Data]</a>
       </li>
      </ul><br>

     <strong>  VLGrammar: Grounded Grammar Induction of Vision and Language</strong><br>
      <ul class="a">
        <li>
          <strong>Yining Hong</strong>; Qing Li; Song-Chun Zhu; Siyuan Huang<br>
          ICCV2021 &nbsp;<a href="https://arxiv.org/abs/2103.12975">[Paper]</a>&nbsp;
          <a href="https://github.com/evelinehong/VLGrammar">[Code & Data]</a>
        </li>
        </ul><br>
      
    <strong>Learning by Fixing: Solving Math Word Problems with Weak Supervision</strong><br>
        <ul class="a">
        <li>
          <strong>Yining Hong</strong>; Qing Li; Daniel Ciao; Siyuan Huang; Song-Chun Zhu<br>
          AAAI2021 &nbsp;<a href="https://evelinehong.github.io/lbf-site">[Project Page]</a>&nbsp;
          <a href="https://arxiv.org/pdf/2012.10582.pdf">[Paper]</a>&nbsp;
          <a href="https://github.com/evelinehong/LBF">[Code]</a>&nbsp;
          <a href="https://evelinehong.github.io/lbf-site/file/AAAI2021%20-%20Learning%20by%20Fixing%20Solving%20Math%20Word%20Problems%20with%20Weak%20Supervision.pdf">[Slides]</a>
        </li>
          </ul><br>
          
     <strong>SMART: A Situation Model for Algebra Story Problems via Attributed Grammar</strong><br>
        <ul class="a">
        <li>
          <strong>Yining Hong</strong>; Qing Li; Ran Gong; Daniel Ciao; Siyuan Huang; Song-Chun Zhu<br>
          AAAI2021 &nbsp;<a href="https://evelinehong.github.io/smart-site">[Project Page]</a>&nbsp;
          <a href="https://arxiv.org/pdf/2012.14011.pdf">[Paper]</a>&nbsp;
          <a href="https://github.com/evelinehong/SMART">[Code & Data]</a>&nbsp;
          <a href="https://evelinehong.github.io/smart-site/file/SMART%20A%20Situation%20Model%20for%20Algebra%20Story%20Problems%20via%20Attributed%20Grammar(1).pdf">[Slides]</a>
        </li>
          </ul><br>

  <strong>Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks</strong><br>
        <ul class="a">
        <li>
          Jinghui Qin; Xiaodan Liang; <strong>Yining Hong</strong>; Jianheng Tang; Liang Lin<br>
          AAAI2021 &nbsp;
          <a href="https://evelinehong.github.io/assets/papers/Neural_Symbolic_Solver_for_Math_Word_Problems_with_Auxiliary_Tasks.pdf">[Paper]</a>&nbsp;
          <a href="https://github.com/evelinehong/SMART">[Code]</a>&nbsp;
        </li>
          </ul><br>

  <strong>A Competence-aware Curriculum for Visual Concepts Learning via Question Answering</strong><br>
        <ul class="a">
        <li>
          Qing Li; Siyuan Huang; <strong>Yining Hong</strong>; Song-Chun Zhu<br>
          ECCV2020 (<b>Oral</b>) &nbsp;
          <a href="https://arxiv.org/abs/2007.01499">[Paper]</a>
        </li>
          </ul><br>
  
  <strong>Closed Loop Neural-Symbolic Learning via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning</strong><br>
        <ul class="a">
        <li>
          Qing Li; Siyuan Huang; <strong>Yining Hong</strong>; Yixin Chen; Yingnian Wu; Song-Chun Zhu<br>
          ICML2020 &nbsp;
          <a href="https://liqing-ustc.github.io/NGS">[Project Page]</a>&nbsp;
          <a href="https://github.com/liqing-ustc/NGS">[Code]</a>
          <br>
          <b>Best Paper Award</b> in ICML 2020 Workshop on Bridge Between Perception and Reasoning: Graph Neural Networks & Beyond
        </li>
          </ul><br>
  
    <strong>Academic Reader: An Interactive Question Answering System on Academic Literatures</strong><br>
        <ul class="a">
        <li>
          <strong>Yining Hong</strong>; Jialu Wang; Yuting Jia; Weinan Zhang; Xinbing Wang<br>
          AAAI2019 (Demo) &nbsp;
          <a href="file:///home/evelyn/Downloads/5065-Article%20Text-8128-1-10-20190709.pdf">[Project Page]</a>&nbsp;
        </li>
          </ul><br>
  </p> 
  
  <p>
    <h2>Awards</h2>
    <ul class="a">
      
    <li>Baidu Scholarship (<b>10 recipients worldwide</b>), 2022.</li>
    <li>Snap Fellowship Honorable Mention, 2022.</li>
    <li>China National Scholarship (<b>Top 0.2%</b>), 2018.</li>
    </ul>
  </p>

    <p>
      
      <h2>Invited Talks</h2>    
        <ul>
      <li> 2023/12 Invited talk "Building General-Purpose Embodied Foundation Models" at Nvidia.</li>
      <li> 2023/12 Invited talk "Building General-Purpose Embodied Foundation Models" at Shanghai Jiao Tong University.</li>
      <li> 2023/12 Invited talk "Building General-Purpose Embodied Foundation Models" at Shanghai AI Lab.</li>
      <li> 2022/4 Invited talk "When Structure-Based Representations Meet Cognitive Reasoning" at <a href="https://vita-group.github.io/research.html">VITA @ UT Austin</a></li>
      <li> 2022/3 Invited talk "Part-based Conceptual, Relational, and Physical Reasoning" at AI Time.</li>
      <li> 2021/12 Invited talk "When Structure-Based Representations Meet Cognitive Reasoning" at <a href="https://www.bigai.ai/">BIGAI</a></li>
      <li> 2021/08 Invited talk "Grounded Grammar Induction of Vision and Language" at AI Drive.</a></li>
        </ul>
    </p> 
<!-- 
  <p> 
    <h2>Research Experience</h2>
  Research Intern. MIT-IBM Watson AI Lab. Supervisor: <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>. 2021.3 - present<br><br>
  Research Assistant. VCLA, UCLA. Supervisor: <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>. 2019.9 - present<br><br>
  Research Intern. Tencent AI Lab. Supervisor: <a href="http://lipiji.com/">Piji Li</a>. 2019.3-2019.7<br><br>
  Research Assitant. IIoT, SJTU. Supervisors: <a href="https://www.cs.sjtu.edu.cn/~wang-xb/">Xinbing Wang</a>, <a href="http://wnzhang.net/">Weinan Zhang</a>. 2016-2019<br>
  </p> -->
  <p>
    <h2>Misc.</h2>
    Outside of research, I'm a multi-instrumentalist. I play the piano, pipa (a Chinese instrument), and harpsichord in an orchestra; the pipe organ in a local church; the keyboard and guitar in a rock band. I also occasionally play the saxophone and melodica (highly underrated instrument!) in casual jazz jams. (Favorite music genres: metal, rock, jazz, classic. <a href="https://pleasekillme.com/">Favorite book.</a>)<br><br>
    I've been playing the piano since I was five, and I was the president of the piano association in Shanghai Jiao Tong University. 
    I really enjoy playing double-piano works with others (In fact I held a double-piano concert when I was an undergraduate!). <br><br>
<!--     I love water sports. I am an advanced scuba diver and a free diver. -->
  </p>

</section>
