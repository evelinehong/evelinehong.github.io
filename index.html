---
layout: default
title: Home
---
<header>
  <img alt="{{ site.name }} picture" src="{{ site.picture }}" />
  <h1>
    <strong>Yining Hong</strong>
  </h1>

  
  <p>
    Stanford University</br>
    Department of Computer Science<br>
  </p>
  <br><br>
  <nav>
    <ul>
<!--       <li><a href="https://drive.google.com/file/d/1mBJ14ZlqIxVOYjJxZRQlTpHOiclSnYRE/view?usp=sharing">CV</a></li> -->
      <li><a href="https://github.com/evelinehong">Github</a></li>
      <li><a href="https://scholar.google.com/citations?user=PTYxORcAAAAJ">Google Scholar</a></li>
      <li><a href="https://twitter.com/yining_hong">Twitter</a></li>
      <li><a href="#music">Music</a></li>
      <li><a href="mailto:yninghong@gmail.com">yninghong AT gmail DOT com</a></li>
<!--       <li><a href="https://evelinehong.github.io/photos.html">Gallery</a></li> -->
    </ul>
  </nav>

  </small>
</header>

<section class="content">
  <p>
    <h2>About Me</h2>

    I am a postdoc at Stanford, advised by <a href="https://yejinc.github.io/">Prof. Yejin Choi</a>. I got my PhD degree in computer science at UCLA. I was fortunate to be co-advised by two of the best advisors in the world, <a href="http://www.stat.ucla.edu/~ywu/">Prof. Ying Nian Wu</a> and <a href="https://web.cs.ucla.edu/~kwchang/">Prof. Kai-Wei Chang</a>. I was also supervised by Prof. Song-Chun Zhu in my early PhD years. Previously, I was an undergraduate in the Department of Electrical Engineering in Shanghai Jiao Tong University, advised by Prof. Xinbing Wang and Prof. Weinan Zhang. I'm generally super grateful for all of my past and current advisors.<br><br>
    <br>
  </p>

  <p>
    <h2> Research Highlights</h2>
    I'm dedicated to building <strong>general-purpose embodied agents</strong> that could actively explore and interact with the <strong>3D physical world</strong>, 
    and perform common sense reasoning within the embodied environment. Specifically, I think the critical aspects of building such embodied agents reside in:
    <ul>
      <li><strong>Building 3D world model</strong>. </li>
      <li><strong>Large embodied foundation models</strong> .</li>
      <li><strong>Visual Common Sense Reasoning</strong>. </li>
    </ul>
<!--     Specifically, I'm interested in: <br>
    <ul>
      <li>Embodied Foundation Models</li>
      <li>3D Reasoning</li>
      <li>Visual Common Sensen Reasoning</li>
      My main focus lies in <strong>3D reasoning</strong>, especially about the perspectives that go beyond pattern recognition in 2D images and need to be reasoned in the 3D space, including physics, functionality etc. I'm also interested in concept learning and structure learning, which are crucial aspects of visual reasoning in the 3D space.<br>
 -->
  </p>
       
<!--   I’ve been drawing inspiration from cognitive science, and endowing machines with human-like concept learning abilities via neural-graphical-symbolic learning. Specifically:<br>
    <strong>Neural</strong>: neural networks for natural language processing and computer vision. <br>
    <strong>Graphical</strong>: graph parsing via grammar learning. <br>
    <strong>Symbolic</strong>: symbolic reasoning, including program synthesis, inductive logic programming and theorem proving. -->
  <!--     My research interest lies in Neural-Symbolic Learning, Mathematical Reasoning and Program Synthesis, particularly in the intersection of Natural Language Processing and Computer Vision. -->
  

  <p>
    <h2>News</h2>
    <ul>
      <li>2025/01: SlowFast-VGen is accepted by ICLR as Spotlight with scores of 8-8-8-6!</li>
      <li>2023/12: I gave a talk on "Building Embodied 3D Foundation Models" at Nvidia!</li>
      <!--<li>2023/12: I will be giving a series of talks on "Building Embodied 3D Foundation Models" at Nvidia (12/18), SJTU (12/28), Shanghai AI Lab (12/29), NUAA (01/05), Peking University (01/08), BIGAI (01/08) and Tsinghua University (01/09)!
      -->
      <li>2023/09: 3D-LLM is accepted by NeurIPS as Spotlight!</li>
      <li>2023/06: Successfully hosted the second <a href=https://mvcs-workshop.github.io/>Machine Visual Common Sense</a> Workshop at CVPR!</li>
    </ul>
  <p>

    <h2>Publications</h2>

EMBODIED WEB AGENTS: Bridging Physical-Digital Realms for Integrated Agent Intelligence<br>
<ul class="a">
    <li>
        <strong>Yining Hong</strong>*; Rui Sun*; Bingxuan Li†; Xingcheng Yao†; Maxine Wu†; Alexander Chien†; Da Yin; Ying Nian Wu; Zhecan James Wang; Kai-Wei Chang <br>
        Neurips 2025 Dataset & Benchmarks (<b>Spotlight</b>);
        <a href="https://embodied-web-agent.github.io/">[Project Page]</a>&nbsp; 
        <a href="https://arxiv.org/pdf/2506.15677">[Paper]</a>&nbsp;
        <a href="https://embodied-web-agent.github.io/">[Data]</a>&nbsp;
        <a href="https://github.com/Embodied-Web-Agent/Embodied-Web-Agent">[Code]</a>
    </li>
</ul>
<br>
  
3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model <br>
<ul class="a">
    <li>
        Wenbo Hu; <strong>Yining Hong</strong>; Yanjun Wang; Leison Gao; Zibu Wei; Xingcheng Yao; Nanyun Peng; Yonatan Bitton; Idan Szpektor; Kai-Wei Chang <br>
        NeurIPS 2025 & CVPR 2025 Foundation Models Meet Embodied Agents Workshop (<b>Best Paper Award</b>)&nbsp;<a href="https://3dllm-mem.github.io/">[Project Page]</a>&nbsp; 
       <a href="https://arxiv.org/abs/2505.22657">[Paper]</a>
    </li>
</ul>
<br>
    
    SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation <br>
<ul class="a">
    <li>
        <strong>Yining Hong</strong>; Beide Liu; Maxine Wu; Yuanhao Zhai; Kai-Wei Chang; Lingjie Li; Kevin Lin; Chung-Ching Lin; Jianfeng Wang; Zhengyuan Yang<sup>††</sup>; Yingnian Wu<sup>††</sup>; Lijuan Wang<sup>††</sup> <br>
        ICLR 2025 (<b>Spotlight, Score 8-8-8-6</b>)&nbsp;<a href="https://slowfast-vgen.github.io">[Project Page]</a>&nbsp; 
       <a href="https://arxiv.org/abs/2410.23277">[Paper]</a>
    </li>
</ul>
<br>
  
    MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World <br>
    <ul class="a">
        <li>
        <strong>Yining Hong</strong>; Zishuo Zheng;  Peihao Chen; Yian Wang; Junyan Li; Chuang Gan <br>
        CVPR 2024 &nbsp;  <a href="https://vis-www.cs.umass.edu/multiply/">[Project Page]</a>&nbsp;  
        <a href="https://arxiv.org/abs/2401.08577">[Paper]</a>
        </li>
      </ul>
        <br>
    
    Compositional VLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding <br>
      <ul class="a">
        <li>
        Junyan Li; Delin Chen; <strong>Yining Hong</strong>; Zhenfang Chen, Peihao Chen; Yikang Shen; Chuang Gan <br>
        ICLR 2024 &nbsp;  <a href="https://vis-www.cs.umass.edu/CoVLM/">[Project Page]</a>&nbsp;  
        <a href="https://arxiv.org/abs/2311.03354">[Paper]</a>&nbsp;  
        <a href="https://github.com/UMass-Foundation-Model/CoVLM">[Code & Data]</a>
        </li>
      </ul>
        <br>

   Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules <br>
      <ul class="a">
        <li>
        Zhenfang Chen; Rui Sun; Wenjun Liu; <strong>Yining Hong</strong>; Chuang Gan <br>
        ICLR 2024&nbsp;  <a href="https://vis-www.cs.umass.edu/genome/">[Project Page]</a>&nbsp;  
        <a href="https://arxiv.org/abs/2311.04901">[Paper]</a>&nbsp;  
        <a href="https://github.com/UMass-Foundation-Model/genome">[Code]</a>&nbsp;
        <a href="https://connecthkuhk-my.sharepoint.com/personal/u3004417_connect_hku_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fu3004417%5Fconnect%5Fhku%5Fhk%2FDocuments%2Fdata%2Fdataset%2Ezip&parent=%2Fpersonal%2Fu3004417%5Fconnect%5Fhku%5Fhk%2FDocuments%2Fdata&ga=1">[Data]</a>
        </li>
      </ul>
        <br>
  
    3D-LLM: Injecting the 3D World into Large Language Models <br>
      <ul class="a">
        <li>
        <strong>Yining Hong</strong>; Haoyu Zhen; Peihao Chen; Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan<br>
        NeurIPS2023 (<b>Spotlight</b>)&nbsp;  <a href="https://vis-www.cs.umass.edu/3dllm/">[Project Page]</a>&nbsp;  
        <a href="https://arxiv.org/abs/2307.12981">[Paper]</a>&nbsp;  
        <a href="https://github.com/UMass-Foundation-Model/3D-LLM">[Code & Data]</a>
        </li>
      </ul>
        <br>
        

    3D Concept Learning and Reasoning from Multi-View Images<br>
    <ul class="a">
      <li>
      <strong>Yining Hong</strong>; Chunru Lin; Yilun Du; Zhenfang Chen; Joshua B. Tenenbaum; Chuang Gan<br>
      CVPR2023&nbsp; <a href="https://vis-www.cs.umass.edu/3d-clr/">[Project Page]</a>&nbsp; 
      <a href="https://arxiv.org/abs/2303.11327">[Paper]</a>&nbsp; 
      <a href="https://github.com/evelinehong/3D-CLR-Official">[Code & Data]</a>
        </li>
      </ul>
    <br>

    See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning<br>
    <ul class="a">
      <li>
    Zhenfang Chen, Qinhong Zhou, Yikang Shen, <strong>Yining Hong</strong>, Hao Zhang, Chuang Gan<br>
    AAAI2024&nbsp; <a href="https://arxiv.org/abs/2301.05226">[Paper]</a>
        </li>
      </ul>
    <br>

    A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics<br>
     <ul class="a">
      <li>
    Qing Li; Siyuan Huang; <strong>Yining Hong</strong>; Yixin Zhu; Ying Nian Wu; Song-Chun Zhu<br>
    ICLR2023 &nbsp;<a href="https://liqing-ustc.github.io/HINT/">[Project Page]</a>&nbsp;
    <a href="https://arxiv.org/abs/2103.01403">[Paper]</a>&nbsp;
    <a href="https://github.com/liqing-ustc/HINT">[Code]</a>&nbsp;
    <a href="https://github.com/liqing-ustc/HINT/raw/master/data/dataset.tar.xz">[Data]</a>
         </li>
      </ul><br>

    3D Concept Grounding on Neural Fields<br>
  <ul class="a">
     <li>
    <strong>Yining Hong</strong>; Yilun Du; Chunru Lin; Joshua Tenenbaum; Chuang Gan<br>
    NeurIPS2022 &nbsp;<a href="https://arxiv.org/abs/2207.06403">[Paper]</a>&nbsp;
    <a href="https://github.com/evelinehong/3D-Concept-Grounding">[Code & Data]</a>
        </li>
      </ul><br>

    Fixing Malfunctional Objects With Learned Physical Simulation and Functional Prediction<br>
  <ul class="a">
     <li>
    <strong>Yining Hong</strong>; Kaichun Mo; Li Yi; Leonidas Guibas; Antonio Torralba; Joshua Tenenbaum; Chuang Gan<br>
    CVPR2022 &nbsp;<a href="https://arxiv.org/abs/2205.02834">[Paper]</a>&nbsp;
    <a href="https://github.com/evelinehong/FixIt">[Code & Data]</a>
       </li>
      </ul><br>
    
  PTR: A Benchmark for Part-based Conceptual, Relational, and Physical Reasoning<br>
  <ul class="a">
     <li>
    <strong> Yining Hong</strong>; Li Yi; Joshua B. Tenenbaum; Antonio Torralba; Chuang Gan<br>
    NeurIPS2021 &nbsp;<a href="https://arxiv.org/abs/2112.05136">[Paper]</a>&nbsp;
    <a href="https://github.com/evelinehong/PTR">[Code & Data]</a>
       </li>
      </ul><br>

    VLGrammar: Grounded Grammar Induction of Vision and Language<br>
      <ul class="a">
        <li>
          <strong>Yining Hong</strong>; Qing Li; Song-Chun Zhu; Siyuan Huang<br>
          ICCV2021 &nbsp;<a href="https://arxiv.org/abs/2103.12975">[Paper]</a>&nbsp;
          <a href="https://github.com/evelinehong/VLGrammar">[Code & Data]</a>
        </li>
        </ul><br>
      
    Learning by Fixing: Solving Math Word Problems with Weak Supervision<br>
        <ul class="a">
        <li>
          <strong>Yining Hong</strong>; Qing Li; Daniel Ciao; Siyuan Huang; Song-Chun Zhu<br>
          AAAI2021 &nbsp;<a href="https://evelinehong.github.io/lbf-site">[Project Page]</a>&nbsp;
          <a href="https://arxiv.org/pdf/2012.10582.pdf">[Paper]</a>&nbsp;
          <a href="https://github.com/evelinehong/LBF">[Code]</a>&nbsp;
          <a href="https://evelinehong.github.io/lbf-site/file/AAAI2021%20-%20Learning%20by%20Fixing%20Solving%20Math%20Word%20Problems%20with%20Weak%20Supervision.pdf">[Slides]</a>
        </li>
          </ul><br>
          
     SMART: A Situation Model for Algebra Story Problems via Attributed Grammar<br>
        <ul class="a">
        <li>
          <strong>Yining Hong</strong>; Qing Li; Ran Gong; Daniel Ciao; Siyuan Huang; Song-Chun Zhu<br>
          AAAI2021 &nbsp;<a href="https://evelinehong.github.io/smart-site">[Project Page]</a>&nbsp;
          <a href="https://arxiv.org/pdf/2012.14011.pdf">[Paper]</a>&nbsp;
          <a href="https://github.com/evelinehong/SMART">[Code & Data]</a>&nbsp;
          <a href="https://evelinehong.github.io/smart-site/file/SMART%20A%20Situation%20Model%20for%20Algebra%20Story%20Problems%20via%20Attributed%20Grammar(1).pdf">[Slides]</a>
        </li>
          </ul><br>

  Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks<br>
        <ul class="a">
        <li>
          Jinghui Qin; Xiaodan Liang; <strong>Yining Hong</strong>; Jianheng Tang; Liang Lin<br>
          AAAI2021 &nbsp;
          <a href="https://evelinehong.github.io/assets/papers/Neural_Symbolic_Solver_for_Math_Word_Problems_with_Auxiliary_Tasks.pdf">[Paper]</a>&nbsp;
          <a href="https://github.com/evelinehong/SMART">[Code]</a>&nbsp;
        </li>
          </ul><br>

  A Competence-aware Curriculum for Visual Concepts Learning via Question Answering<br>
        <ul class="a">
        <li>
          Qing Li; Siyuan Huang; <strong>Yining Hong</strong>; Song-Chun Zhu<br>
          ECCV2020 (<b>Oral</b>) &nbsp;
          <a href="https://arxiv.org/abs/2007.01499">[Paper]</a>
        </li>
          </ul><br>
  
  Closed Loop Neural-Symbolic Learning via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning<br>
        <ul class="a">
        <li>
          Qing Li; Siyuan Huang; <strong>Yining Hong</strong>; Yixin Chen; Yingnian Wu; Song-Chun Zhu<br>
          ICML2020 &nbsp;
          <a href="https://liqing-ustc.github.io/NGS">[Project Page]</a>&nbsp;
          <a href="https://github.com/liqing-ustc/NGS">[Code]</a>
          <br>
          <b>Best Paper Award</b> in ICML 2020 Workshop on Bridge Between Perception and Reasoning: Graph Neural Networks & Beyond
        </li>
          </ul><br>
  
    Academic Reader: An Interactive Question Answering System on Academic Literatures<br>
        <ul class="a">
        <li>
          <strong>Yining Hong</strong>; Jialu Wang; Yuting Jia; Weinan Zhang; Xinbing Wang<br>
          AAAI2019 (Demo) &nbsp;
          <a href="file:///home/evelyn/Downloads/5065-Article%20Text-8128-1-10-20190709.pdf">[Project Page]</a>&nbsp;
        </li>
          </ul><br>
  </p> 
  
  <p>
    <h2>Awards & Honors</h2>
    <ul class="a">
    <li>Two Sigma PhD Fellowship Award UCLA Internal Nomination, 2024. </li>
    <li>CVPR PhD Consortium Award, 2024.</li>
    <li>Rising Stars in Computational and Data Sciences, 2024.</li>
    <li>Baidu Scholarship (<b>10 recipients worldwide</b>), 2022.</li>
    <li>Snap Fellowship Honorable Mention, 2022.</li>
    <li>China National Scholarship (<b>Top 0.2%</b>), 2018.</li>
    </ul>
  </p>

    <p>
      
      <h2>Invited Talks</h2>    
        <ul>
      <li> 2023/04 Invited talk at Umass Amherst</li>
      <li> 2023/02 Invited talks on "Building Embodied 3D Foundation Models" at Apple, USC and CUHK</li>
      <li> 2024/01 Invited talks on "Building Embodied 3D Foundation Models" at Tsinghua University, Peking University</li>
      <li> 2023/12 Invited talks "Building General-Purpose Embodied Foundation Models" at Nvidia, Shanghai Jiao Tong University and Shanghai AI Lab</li>
      <li> 2022/4 Invited talk "When Structure-Based Representations Meet Cognitive Reasoning" at UT Austin</a></li>
      <li> 2022/3 Invited talk "Part-based Conceptual, Relational, and Physical Reasoning" at AI Time.</li>
      <li> 2021/08 Invited talk "Grounded Grammar Induction of Vision and Language" at AI Drive.</a></li>
        </ul>
    </p> 
<!-- 
  <p> 
    <h2>Research Experience</h2>
  Research Intern. MIT-IBM Watson AI Lab. Supervisor: <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>. 2021.3 - present<br><br>
  Research Assistant. VCLA, UCLA. Supervisor: <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>. 2019.9 - present<br><br>
  Research Intern. Tencent AI Lab. Supervisor: <a href="http://lipiji.com/">Piji Li</a>. 2019.3-2019.7<br><br>
  Research Assitant. IIoT, SJTU. Supervisors: <a href="https://www.cs.sjtu.edu.cn/~wang-xb/">Xinbing Wang</a>, <a href="http://wnzhang.net/">Weinan Zhang</a>. 2016-2019<br>
  </p> -->
  <p>
    <h2 id="music">As A Musician</h2>
        <li>2025/6, I played the electric guitar / banjo / keyboard / melodica / percussion set and sang in CVPR reception.</li>
    <li>2025/8, I will play the electric guitar, organ, banjo and sing in San Jose Jazz Festival and Magwest Music Festival.</li>
    <li>2025/8, I joined <a href="https://www.turnmeondead.com/"> Turn Me On Dead</a> as a synth keyboardist.</li>
<!--     <br>
    You can find my musician works <a href ="https://www.instagram.com/eveline.here/">here</a> -->
    <br><br>
    Outside of research, I'm a multi-instrumentalist, composer, and <strong>metalhead</strong>. I was the featured organist, keyboard section leader, pipa soloist, ondes martenot soloist in an orchestra (ondes martenot is a very rare instrument - only 60 ondes in total worldwide). I also play(ed) the pipe organ in churches; the keyboard and guitar in a rock band; occasionally play the saxophone, melodica, drums, harp,  Konghou, in casual jams. (<a href="https://pleasekillme.com/">Favorite book.</a> Favorite Band: Dream Theater. Favorite Album: Metropolis Pt 2.)<br><br>
    I've been playing the piano since I was five, and I was the president of the piano association in Shanghai Jiao Tong University. I can do fast sightreading on staff / lead sheet, and have rich recording / performing experience on piano, organ and pipa. I'm a Young Organist of American Guild of Organists and my organ teacher was Prof. Christoph Bull from UCLA. (Church organist experience: sub-organist at First Baptist Church of Glendale -> sub-organist at St Mark's Episcopal Church in Palo Alto & First Congregational San Jose.)  
<!--     Right now I'm really into improvisation and composition. <strong>I feel tons of music inside me and the urge to express it</strong>. Improvisation also helps me understand more about human planning ^^ <br><br> -->
    

<!--     I love water sports. I am an advanced scuba diver and a free diver. -->
  </p>

</section>
